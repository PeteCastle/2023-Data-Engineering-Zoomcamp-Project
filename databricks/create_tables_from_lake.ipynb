{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50a8ed5f-c271-44d7-b4ea-6c4fe839f16d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, ByteType, ShortType, BooleanType, TimestampType,LongType\n",
    "from pyspark.sql.functions import col, to_timestamp, expr\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# from databricks.connect import DatabricksSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = json.load(open('../credentials/credentials.json'))\n",
    "configs = json.load(open('../configs/flows_config.json'))\n",
    "\n",
    "storage_account_name = credentials[\"AZURE_BLOB_PROJECT_NAME\"]\n",
    "storage_account_access_key = credentials[\"AZURE_BLOB_STORAGE_KEY\"]\n",
    "blob_container = credentials[\"AZURE_BLOB_CONTAINER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d089fe5e-5132-4db7-8be1-78d46df0bdbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark = DatabricksSession.builder.remote(\n",
    "#     host = credentials[\"SPARK_MASTER_HOSTNAME\"],\n",
    "#     token = credentials[\"DATABRICKS_ACCESS_TOKEN\"],\n",
    "#     cluster_id = credentials[\"DATABRICKS_CLUSTER_ID\"]\n",
    "# ).getOrCreate()\n",
    "\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "    # .appName(\"Main Spark App\")\\\n",
    "    # .config(\"spark.master\", SPARK_MASTER_CONNECTION_STRING) \\\n",
    "    # .config(\"fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\\\n",
    "    # .config(f'fs.azure.account.key.{storage_account_name}.blob.core.windows.net',storage_account_access_key)\\\n",
    "    # .getOrCreate()\n",
    "\n",
    "    spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04338853-57e1-47dc-8e63-bee3c99a19df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Accounts DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a669ad85-8868-4910-aa9e-82cfc54db6cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Read, Parse, and Clean Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7577cea1-73f2-4669-9523-0a8e0dc89a70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "SparkConnectGrpcException",
     "evalue": "<_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"BAD_REQUEST: Spark Connect is enabled only on Unity Catalog enabled Shared and Single User Clusters.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:52.187.145.107:443 {created_time:\"2023-04-26T08:29:26.5990023+00:00\", grpc_status:3, grpc_message:\"BAD_REQUEST: Spark Connect is enabled only on Unity Catalog enabled Shared and Single User Clusters.\"}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSparkConnectGrpcException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Educational Others\\2023 Data Engineering Zoomcamp Project\\databricks\\create_tables_from_lake.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Educational%20Others/2023%20Data%20Engineering%20Zoomcamp%20Project/databricks/create_tables_from_lake.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m accounts_df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mcsv\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mheader\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mload(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwasbs://\u001b[39m\u001b[39m{\u001b[39;00mblob_container\u001b[39m}\u001b[39;00m\u001b[39m@\u001b[39m\u001b[39m{\u001b[39;00mstorage_account_name\u001b[39m}\u001b[39;00m\u001b[39m.blob.core.windows.net/resources/accounts/*.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Educational%20Others/2023%20Data%20Engineering%20Zoomcamp%20Project/databricks/create_tables_from_lake.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m accounts_df \u001b[39m=\u001b[39m accounts_df\u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mrevisionDate\u001b[39m\u001b[39m\"\u001b[39m, to_timestamp(col(\u001b[39m\"\u001b[39m\u001b[39mrevisionDate\u001b[39m\u001b[39m\"\u001b[39m))) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Educational%20Others/2023%20Data%20Engineering%20Zoomcamp%20Project/databricks/create_tables_from_lake.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                 \u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mprofileIconId\u001b[39m\u001b[39m\"\u001b[39m, col(\u001b[39m\"\u001b[39m\u001b[39mprofileIconId\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mcast(ShortType())) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Educational%20Others/2023%20Data%20Engineering%20Zoomcamp%20Project/databricks/create_tables_from_lake.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                 \u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39msummonerLevel\u001b[39m\u001b[39m\"\u001b[39m, col(\u001b[39m\"\u001b[39m\u001b[39msummonerLevel\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mcast(ShortType())) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Educational%20Others/2023%20Data%20Engineering%20Zoomcamp%20Project/databricks/create_tables_from_lake.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                 \u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mleaguePoints\u001b[39m\u001b[39m\"\u001b[39m, col(\u001b[39m\"\u001b[39m\u001b[39mleaguePoints\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mcast(ShortType())) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Educational%20Others/2023%20Data%20Engineering%20Zoomcamp%20Project/databricks/create_tables_from_lake.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                 \u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mwins\u001b[39m\u001b[39m\"\u001b[39m, col(\u001b[39m\"\u001b[39m\u001b[39mwins\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mcast(ShortType())) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Educational%20Others/2023%20Data%20Engineering%20Zoomcamp%20Project/databricks/create_tables_from_lake.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                 \u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mlosses\u001b[39m\u001b[39m\"\u001b[39m, col(\u001b[39m\"\u001b[39m\u001b[39mlosses\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mcast(ShortType()))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Educational%20Others/2023%20Data%20Engineering%20Zoomcamp%20Project/databricks/create_tables_from_lake.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m accounts_df\u001b[39m.\u001b[39;49mschema\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pyspark\\sql\\connect\\dataframe.py:1375\u001b[0m, in \u001b[0;36mDataFrame.schema\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_session \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot analyze without SparkSession.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1375\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mschema(query)\n\u001b[0;32m   1376\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mEmpty plan.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pyspark\\sql\\connect\\client.py:732\u001b[0m, in \u001b[0;36mSparkConnectClient.schema\u001b[1;34m(self, plan)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    729\u001b[0m \u001b[39mReturn schema for given plan.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    731\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSchema for plan: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_proto_to_string(plan)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 732\u001b[0m schema \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_analyze(method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mschema\u001b[39;49m\u001b[39m\"\u001b[39;49m, plan\u001b[39m=\u001b[39;49mplan)\u001b[39m.\u001b[39mschema\n\u001b[0;32m    733\u001b[0m \u001b[39massert\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \u001b[39m# Server side should populate the struct field which is the schema.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pyspark\\sql\\connect\\client.py:881\u001b[0m, in \u001b[0;36mSparkConnectClient._analyze\u001b[1;34m(self, method, **kwargs)\u001b[0m\n\u001b[0;32m    879\u001b[0m     \u001b[39mraise\u001b[39;00m SparkConnectException(\u001b[39m\"\u001b[39m\u001b[39mInvalid state during retry exception handling.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    880\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m rpc_error:\n\u001b[1;32m--> 881\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_error(rpc_error)\n\u001b[0;32m    882\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    883\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ping_exit()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pyspark\\sql\\connect\\client.py:1111\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[1;34m(self, rpc_error)\u001b[0m\n\u001b[0;32m   1109\u001b[0m     \u001b[39mraise\u001b[39;00m SparkConnectGrpcException(status\u001b[39m.\u001b[39mmessage) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m   1110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1111\u001b[0m     \u001b[39mraise\u001b[39;00m SparkConnectGrpcException(\u001b[39mstr\u001b[39m(rpc_error)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mSparkConnectGrpcException\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"BAD_REQUEST: Spark Connect is enabled only on Unity Catalog enabled Shared and Single User Clusters.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:52.187.145.107:443 {created_time:\"2023-04-26T08:29:26.5990023+00:00\", grpc_status:3, grpc_message:\"BAD_REQUEST: Spark Connect is enabled only on Unity Catalog enabled Shared and Single User Clusters.\"}\"\n>"
     ]
    }
   ],
   "source": [
    "accounts_df = spark.read.format(\"csv\").option(\"header\",\"true\").load(f\"wasbs://{blob_container}@{storage_account_name}.blob.core.windows.net/resources/accounts/*.csv\")\n",
    "\n",
    "accounts_df = accounts_df.withColumn(\"revisionDate\", to_timestamp(col(\"revisionDate\"))) \\\n",
    "                .withColumn(\"profileIconId\", col(\"profileIconId\").cast(ShortType())) \\\n",
    "                .withColumn(\"summonerLevel\", col(\"summonerLevel\").cast(ShortType())) \\\n",
    "                .withColumn(\"leaguePoints\", col(\"leaguePoints\").cast(ShortType())) \\\n",
    "                .withColumn(\"wins\", col(\"wins\").cast(ShortType())) \\\n",
    "                .withColumn(\"losses\", col(\"losses\").cast(ShortType()))\n",
    "\n",
    "accounts_df.schema \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a137e4b7-5b4b-4a03-b1fe-306c049dacf1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Cluster and Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6a8fe70-9bca-4a75-8aca-f5821d81da58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS accounts\")\n",
    "accounts_df.write.partitionBy(\"leagueId\").mode(\"overwrite\").saveAsTable(\"accounts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0300c45f-9858-420f-8aab-d90a0de1550f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Performance Test\n",
    "Uncomment the code below if you want to test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d1b6e80-e696-4d45-b996-ddc49fcec28e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# spark.sql(\"DROP TABLE IF EXISTS accounts_not_clustered\")\n",
    "\n",
    "# accounts_df.write.mode(\"overwrite\").saveAsTable(\"accounts_not_clustered\")\n",
    "\n",
    "# start = datetime.now()\n",
    "# test = spark.sql(\"SELECT COUNT(*) FROM accounts GROUP BY leagueId\").show(1)\n",
    "# print(f\"Took {datetime.now() - start} to complete with clustered dataframe\")\n",
    "\n",
    "# start = datetime.now()\n",
    "# test = spark.sql(\"SELECT COUNT(*) FROM accounts_not_clustered GROUP BY leagueId\").show(1)\n",
    "\n",
    "# print(f\"Took {datetime.now() - start } to complete with non paritioned dataframe\")\n",
    "\n",
    "# print(\"Partitions can be at least 25 percent faster.  Similar steps can be reproduced in the following data frame.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e015a18-ba7e-4574-96d9-b7e555de62de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Champion Mastery DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ca596ca-6f4f-4cbf-99de-bdd54e99dc2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "champion_mastery_df = spark.read.format(\"csv\").option(\"header\",\"true\").load(f\"wasbs://{blob_container}@{storage_account_name}.blob.core.windows.net/resources/champion_mastery/*.csv\")\n",
    "\n",
    "champion_mastery_df = champion_mastery_df.withColumn(\"lastPlayTime\", to_timestamp(col(\"lastPlayTime\"))) \\\n",
    "                        .withColumn(\"championId\", col(\"championId\").cast(ShortType())) \\\n",
    "                        .withColumn(\"championLevel\", col(\"championLevel\").cast(ByteType())) \\\n",
    "                        .withColumn(\"tokensEarned\", col(\"tokensEarned\").cast(ByteType())) \\\n",
    "\n",
    "champion_mastery_df.printSchema()\n",
    "spark.sql(\"DROP TABLE IF EXISTS champion_mastery\")\n",
    "champion_mastery_df.write.partitionBy(\"championId\").mode(\"overwrite\").saveAsTable(\"champion_mastery\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e79871d3-d374-4c31-8368-5b3f78373767",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Leagues DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b4c6cfb-75b0-49d7-a4f6-7356b2273f2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "leagues_df = spark.read.format(\"csv\").option(\"header\",\"true\").load(f\"wasbs://{blob_container}@{storage_account_name}.blob.core.windows.net/resources/leagues/*/*/*/*.csv\")\n",
    "leagues_df.drop_duplicates([\"tier\",\"division\"]).show()\n",
    "leagues_df.printSchema()\n",
    "                \n",
    "spark.sql(\"DROP TABLE IF EXISTS leagues\")\n",
    "leagues_df.write.partitionBy(\"tier\").mode(\"overwrite\").saveAsTable(\"leagues\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0046096d-a3f7-472b-b8a9-748dd579aaa8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Dragon DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba5a9dca-4250-4ada-be3e-77874935bf57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "champion_infos_df = spark.read.format(\"parquet\").load(f\"wasbs://{blob_container}@{storage_account_name}.blob.core.windows.net/resources/data_dragon/{configs[\"LEAGUE_PATCH\"]}/champion_infos.parquet\")\n",
    "champion_infos_df = champion_infos_df \\\n",
    "                        .withColumn(\"key\", col(\"key\").cast(ShortType())) \\\n",
    "                        .withColumn(\"attack_stat\", col(\"attack_stat\").cast(ShortType())) \\\n",
    "                        .withColumn(\"defense_stat\", col(\"defense_stat\").cast(ShortType())) \\\n",
    "                        .withColumn(\"magic_stat\", col(\"magic_stat\").cast(ShortType())) \\\n",
    "                        .withColumn(\"difficulty_stat\", col(\"difficulty_stat\").cast(ShortType())) \\\n",
    "                        .withColumn(\"hpperlevel\", col(\"hpperlevel\").cast(ShortType())) \\\n",
    "                        .withColumn(\"base_mp\", col(\"base_mp\").cast(ShortType())) \\\n",
    "                        .withColumn(\"base_armor\", col(\"base_armor\").cast(ShortType())) \\\n",
    "                        .withColumn(\"base_spellblock\", col(\"base_spellblock\").cast(ShortType())) \\\n",
    "                        .withColumn(\"base_attackrange\", col(\"base_attackrange\").cast(ShortType())) \\\n",
    "                        .withColumn(\"base_crit\", col(\"base_crit\").cast(ShortType())) \\\n",
    "                        .withColumn(\"critperlevel\", col(\"critperlevel\").cast(ShortType())) \\\n",
    "                        .withColumn(\"base_attackdamage\", col(\"base_attackdamage\").cast(ShortType())) \\\n",
    "\n",
    "champion_infos_df.printSchema()\n",
    "spark.sql(\"DROP TABLE IF EXISTS champion_info\")\n",
    "champion_infos_df.write.partitionBy(\"primary_class\").mode(\"overwrite\").saveAsTable(\"champion_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67b2c564-2745-4923-82c0-a7d2c47299f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "item_infos_df = spark.read.format(\"parquet\").load(f\"wasbs://{blob_container}@{storage_account_name}.blob.core.windows.net/resources/data_dragon/{configs[\"LEAGUE_PATCH\"]}/item_infos.parquet\")\n",
    "item_infos_df = item_infos_df.withColumn(\"id\", col(\"id\").cast(ShortType()))\\\n",
    "    .withColumn(\"baseGold\", col(\"baseGold\").cast(ShortType()))\\\n",
    "    .withColumn(\"sellGold\", col(\"sellGold\").cast(ShortType()))\\\n",
    "    .withColumn(\"totalGold\", col(\"totalGold\").cast(ShortType()))\\\n",
    "    .withColumn(\"maximumStacks\", col(\"maximumStacks\").cast(ShortType()))\\\n",
    "    .withColumn(\"depth\", col(\"depth\").cast(ShortType()))\\\n",
    "    .withColumn(\"specialRecipe\", col(\"specialRecipe\").cast(ShortType()))\n",
    "\n",
    "item_infos_df.printSchema()\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS item_info\")\n",
    "item_infos_df.write.mode(\"overwrite\").saveAsTable(\"item_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "211912b7-daaf-4fac-83e9-36d25f9b5557",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS map_info\")\n",
    "map_infos_df = spark.read.format(\"parquet\").load(f\"wasbs://{blob_container}@{storage_account_name}.blob.core.windows.net/resources/data_dragon/{configs[\"LEAGUE_PATCH\"]}/map_infos.parquet\") \\\n",
    "    .write.mode(\"overwrite\").saveAsTable(\"map_info\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b343b16e-c4fb-485f-b238-ea96dfaf22f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rune_infos_df = spark.read.format(\"parquet\").load(f\"wasbs://{blob_container}@{storage_account_name}.blob.core.windows.net/resources/data_dragon/{configs[\"LEAGUE_PATCH\"]}/rune_infos.parquet\")\n",
    "rune_infos_df = rune_infos_df.withColumn(\"id\", col(\"id\").cast(ShortType()))\\\n",
    "             .withColumn(\"rune_level\", col(\"rune_level\").cast(ShortType()))\\\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS rune_info\")\n",
    "rune_infos_df.write.mode(\"overwrite\").saveAsTable(\"rune_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "907b9f44-a8ec-4b74-97d8-46f36fe84bbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "summoner_spells_info_df = spark.read.format(\"parquet\").load(f\"wasbs://{blob_container}@{storage_account_name}.blob.core.windows.net/resources/data_dragon/{configs[\"LEAGUE_PATCH\"]}/summoner_spells_info.parquet\")\n",
    "summoner_spells_info_df = summoner_spells_info_df.withColumn(\"key\", col(\"key\").cast(ShortType()))\\\n",
    "                        .withColumn(\"maxrank\", col(\"maxrank\").cast(ShortType()))\\\n",
    "                        .withColumn(\"cooldown\", col(\"cooldown\").cast(ShortType()))\\\n",
    "                        .withColumn(\"cooldownBurn\", col(\"cooldownBurn\").cast(ShortType()))\\\n",
    "                        .withColumn(\"cost\", col(\"cost\").cast(ShortType()))\\\n",
    "                        .withColumn(\"minimumSummonerLevel\", col(\"minimumSummonerLevel\").cast(ShortType()))\\\n",
    "                        .withColumn(\"rangeBurn\", col(\"rangeBurn\").cast(ShortType()))\\\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS summoner_spells_info\")\n",
    "summoner_spells_info_df.write.mode(\"overwrite\").saveAsTable(\"summoner_spells_info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69332d2f-79c1-40b0-b719-aca63d4fad19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## General Matches DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "617773c4-adcf-47d4-a501-91408774d761",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "general_match_df = spark.read.format(\"csv\").option(\"header\",\"true\").load(f\"wasbs://{blob_container}@{storage_account_name}.blob.core.windows.net/resources/match/general/*/*/*/*.csv\")\n",
    "\n",
    "general_match_df = general_match_df.withColumn(\"gameCreation\", to_timestamp(col(\"gameCreation\")/1000)) \\\n",
    "            .withColumn(\"gameStartTimestamp\", to_timestamp(col(\"gameStartTimestamp\")/1000)) \\\n",
    "            .withColumn(\"gameEndTimestamp\", to_timestamp(col(\"gameEndTimestamp\")/1000)) \\\n",
    "            .withColumn(\"queueId\", col(\"queueId\").cast(ShortType()))\\\n",
    "            .withColumn(\"mapId\", col(\"mapId\").cast(ShortType()))\\\n",
    "            .withColumn(\"dataVersion\", col(\"dataVersion\").cast(ByteType()))\\\n",
    "            .withColumn(\"gameDuration\", expr(\"interval '1 second' * gameDuration\"))\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS general_match\")\n",
    "general_match_df.write.partitionBy(\"gameMode\").mode(\"overwrite\").saveAsTable(\"general_match\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66fdf888-6c29-4e10-90d1-93b272d42fdc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "players_match_df = spark.read.format(\"csv\").option(\"header\",\"true\").load(f\"wasbs://{blob_container}@{storage_account_name}.blob.core.windows.net/resources/match/players/*/*/*/*.csv\")\n",
    "\n",
    "for column, col_type in players_match_df.dtypes:\n",
    "    # To Fix, challenges df not removed in ETL process: match_entries.py:getMatchPlayersInfo\n",
    "    if \"challenges\" in column:\n",
    "        players_match_df = players_match_df.drop(column)\n",
    "    if col_type == \"bigint\":\n",
    "        players_match_df = players_match_df.withColumn(column, col(column).cast(ShortType()))\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS players_match\")\n",
    "players_match_df.write.partitionBy(\"championId\").mode(\"overwrite\").saveAsTable(\"players_match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6374ee93-af5f-48a4-8671-e8808bd3ca4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\",\"true\")\n",
    "players_challenges_match_df = spark.read.format(\"csv\").option(\"header\",\"true\").load(f\"wasbs://{blob_container}@{storage_account_name}.blob.core.windows.net/resources/match/players_challenges/*/*/*/*.csv\")\n",
    "double_rows = [\"damagePerMinute\",\n",
    "\"damageTakenOnTeamPercentage\",\n",
    "\"effectiveHealAndShielding\",\n",
    "\"gameLength\",\n",
    "\"goldPerMinute\",\n",
    "\"kda\",\n",
    "\"killParticipation'\",\n",
    "\"shortestTimeToAceFromFirstTakedown\",\n",
    "\"teamDamagePercentage\"]\n",
    "\n",
    "\n",
    "for column, col_type in players_challenges_match_df.dtypes:\n",
    "    if (col_type == \"bigint\" or col_type ==\"double\") and col_type not in double_rows:\n",
    "        players_challenges_match_df = players_challenges_match_df.withColumn(column, col(column).cast(ShortType()))\n",
    "\n",
    "players_challenges_match_df = players_challenges_match_df.withColumn('alliedJungleMonsterKills', col('alliedJungleMonsterKills').cast(LongType()))\n",
    "\n",
    "#Table has no partitions\n",
    "spark.sql(\"DROP TABLE IF EXISTS player_challenges_match\")\n",
    "players_challenges_match_df.write.mode(\"overwrite\").saveAsTable(\"player_challenges_match\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a7a9f03-e3b8-4ce6-b068-7d13f6d20767",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "teams_match_df = spark.read.format(\"csv\").option(\"header\",\"true\").load(f\"wasbs://{blob_container}@{storage_account_name}.blob.core.windows.net/resources/match/teams/*/*/*/*.csv\")\n",
    "\n",
    "for column, col_type in teams_match_df.dtypes:\n",
    "    if (col_type == \"bigint\" or col_type ==\"double\"):\n",
    "        teams_match_df = teams_match_df.withColumn(column, col(column).cast(ShortType()))\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS teams_match\")\n",
    "#Table has no partitions\n",
    "teams_match_df.write.mode(\"overwrite\").saveAsTable(\"teams_match\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END OF FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37eee108-2523-4d34-8016-80adbd7b1c83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT matchId, championId FROM players_match ORDER BY championId DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c791f920-ed6c-45d1-b069-de4f6d5a5a20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "\n",
    "     SELECT COUNT(DISTINCT(matchId)) FROM `dbt_petecastle`.`champion_picks_bans`\n",
    "    WHERE type = \"ban\" AND championId = \"Aatrox\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d23cea77-2087-4ac4-9f40-205c3a6fd389",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    WITH temp_table AS (\n",
    "        SELECT * FROM dbt_petecastle.champion_item_picks\n",
    "        WHERE championId = 235 \n",
    "        -- GROUP BY matchId\n",
    "    )\n",
    "    SELECT *\n",
    "    FROM temp_table\n",
    "    ORDER BY matchId\n",
    "    \"\"\").show()\n",
    "\n",
    "# -- 3262\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3db94f4-569c-4826-8eb5-2f1ae0198718",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT \n",
    "            primaryStyle_0_id,\n",
    "            -- primaryStyle_0_var1,\n",
    "            -- primaryStyle_0_var2,\n",
    "            -- primaryStyle_0_var3,\n",
    "            primaryStyle_1_id,\n",
    "            -- primaryStyle_1_var1,\n",
    "            -- primaryStyle_1_var2,\n",
    "            -- primaryStyle_1_var3,\n",
    "            primaryStyle_2_id,\n",
    "            -- primaryStyle_2_var1,\n",
    "            -- primaryStyle_2_var2,\n",
    "            -- primaryStyle_2_var3,\n",
    "            primaryStyle_3_id\n",
    "            -- primaryStyle_3_var1,\n",
    "            -- primaryStyle_3_var2,\n",
    "            -- primaryStyle_3_var3\n",
    "        FROM players_match\n",
    "    \n",
    "    \"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2829335426119789,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "create_tables_from_lake",
   "notebookOrigID": 4465607492480377,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
